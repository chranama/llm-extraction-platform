# config/server.yaml
# ------------------------------------------------------------
# Shape:
#   base:      canonical defaults for all environments
#   profiles:  overlays applied on top of base (selected by APP_PROFILE)
#
# Selection:
#   APP_PROFILE=test|host|docker  (default: host)
# ------------------------------------------------------------

base:
  service:
    name: "LLM Server"
    version: "0.1.0"
    debug: false
    env: "dev"

  server:
    host: "0.0.0.0"
    port: 8000

  api:
    cors_allowed_origins: []

  capabilities:
    generate: true
    extract: true

  model:
    default_id: "mistralai/Mistral-7B-v0.1"
    allowed_models: []
    models_config_path: "config/models.yaml"
    dtype: "float16"
    device: null
    model_load_mode: "lazy"
    require_model_ready: false
    token_counting: true

  redis:
    enabled: false

  # NOTE: Gate is source-of-truth for total budget.
  # Keep client timeouts aligned with gate timeout (or larger, but llm.py clamps per-backend anyway).
  http:
    llm_service_url: "http://127.0.0.1:9001"
    client_timeout_seconds: 30

  limits:
    rate_limit_rpm:
      admin: 0
      default: 120
      free: 30
    quota_auto_reset_days: 30

    # GenerateGate is the total SLA budget (queue wait + execution).
    generate_gate:
      enabled: true
      max_concurrent: 2
      max_queue: 32
      timeout_seconds: 30
      fail_fast: true
      count_queued_as_in_flight: false

    # Fast-path overload protection (treat 0 as "disabled check").
    generate_early_reject:
      enabled: true
      reject_queue_depth_gte: 0
      reject_in_flight_gte: 0
      routes:
        - "/v1/generate"
        - "/v1/generate/batch"

  cache:
    api_key_cache_ttl_seconds: 10


profiles:
  # Demo B: runs on host; llama-server may run on host as separate service.
  host: {}

  # Demo A: runs in docker; likely with redis enabled and model required for readiness.
  docker:
    api:
      cors_allowed_origins:
        - "http://localhost:5173"
        - "http://127.0.0.1:5173"
        - "http://localhost"
        - "http://127.0.0.1"

    redis:
      enabled: true

    model:
      model_load_mode: "eager"
      require_model_ready: true

    # Optional: make early reject more aggressive in docker demos (uncomment if desired)
    # limits:
    #   generate_early_reject:
    #     reject_queue_depth_gte: 24
    #     reject_in_flight_gte: 6

  test:
    service:
      name: "LLM Server (test)"
      env: "test"

    server:
      host: "127.0.0.1"
      port: 8000

    api:
      cors_allowed_origins: []

    capabilities:
      generate: true
      extract: true

    model:
      default_id: "fake"
      allowed_models: []
      models_config_path: "config/models.yaml"
      dtype: "float16"
      device: "cpu"
      model_load_mode: "lazy"
      require_model_ready: false
      token_counting: false

    redis:
      enabled: false

    http:
      client_timeout_seconds: 5

    limits:
      generate_gate:
        enabled: true
        max_concurrent: 16
        max_queue: 0
        timeout_seconds: 2
        fail_fast: true
        count_queued_as_in_flight: false

      generate_early_reject:
        enabled: true
        reject_queue_depth_gte: 0
        reject_in_flight_gte: 0
        routes:
          - "/v1/generate"
          - "/v1/generate/batch"

    cache:
      api_key_cache_ttl_seconds: 1