# config/models.yaml
# ------------------------------------------------------------
# Notes based on our conversation:
# - Gate timeout (server.yaml limits.generate_gate.timeout_seconds) is the source-of-truth budget.
# - llm.py clamps backend HTTP timeouts to (gate_timeout - buffer), so do NOT set 120s here.
# - Keep llamacpp/remote timeout_seconds at <= gate timeout for legibility.
# ------------------------------------------------------------

base:
  default_model: meta-llama/Llama-3.2-1B-Instruct

  defaults:
    backend: transformers
    load_mode: lazy
    capabilities:
      generate: true
      extract: false

    transformers:
      device: auto
      dtype: float16
      trust_remote_code: false
      default_temperature: 0.7
      default_top_p: 0.95

    # Used only when backend=llamacpp
    llamacpp:
      server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8080}
      timeout_seconds: 30
      default_temperature: 0.7
      default_top_p: 0.95
      # model_name: ""  # optional

    # Used only when backend=remote
    remote:
      base_url: ${REMOTE_BACKEND_BASE_URL:-}
      timeout_seconds: 30
      # model_id: ""    # optional

  models:
    - id: meta-llama/Llama-3.2-1B-Instruct
      backend: transformers
      load_mode: lazy
      transformers:
        device: auto
        dtype: float16
        trust_remote_code: false
      capabilities: { generate: true, extract: false }
      notes: "Base default (small, CPU-friendly; generate-only)"

    - id: mistralai/Mistral-7B-Instruct-v0.3
      backend: transformers
      load_mode: lazy
      transformers:
        device: auto
        dtype: float16
      capabilities: { generate: true, extract: false }
      notes: "Optional model (generate-only)"

    - id: Qwen/Qwen2.5-7B-Instruct
      backend: transformers
      load_mode: lazy
      transformers:
        device: auto
        dtype: float16
      capabilities: { generate: true, extract: false }
      notes: "Optional model (generate-only)"

    # ==========================
    # llama.cpp (GGUF) via llama-server (external service)
    # ==========================

    - id: llama.cpp/Meta-Llama-3.1-8B-Instruct-Q4_K_M-GGUF
      backend: llamacpp
      load_mode: lazy
      llamacpp:
        server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
        timeout_seconds: 30
        model_name: "Meta-Llama-3.1-8B-Instruct"
        # documentation-only:
        model_path: "/Users/chranama/models/gguf/llama-3.1-8b-instruct/Q4_K_M.gguf"
      capabilities: { generate: true, extract: false }
      notes: "Host llama-server (llama.cpp) GGUF: Meta-Llama-3.1-8B-Instruct Q4_K_M"

    - id: llama.cpp/Qwen2.5-14B-Instruct-Q4_K_M-GGUF
      backend: llamacpp
      load_mode: lazy
      llamacpp:
        server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
        timeout_seconds: 30
        model_name: "Qwen2.5-14B-Instruct"
        # documentation-only:
        model_path: "/Users/chranama/models/gguf/qwen2.5-14b-instruct/Q4_K_M.gguf"
      capabilities: { generate: true, extract: false }
      notes: "Host llama-server (llama.cpp) GGUF: Qwen2.5-14B-Instruct Q4_K_M"

    - id: llama.cpp/TinyLlama-1.1B-Chat-Q4_K_M-GGUF
      backend: llamacpp
      load_mode: lazy
      llamacpp:
        server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
        timeout_seconds: 30
        model_name: "TinyLlama-1.1B-Chat"
        # documentation-only:
        model_path: "/Users/chranama/models/gguf/tinyllama-1.1b-chat/Q4_K_M.gguf"
      capabilities: { generate: true, extract: false }
      notes: "Host llama-server (llama.cpp) GGUF: TinyLlama 1.1B Chat Q4_K_M (tiny + fast)"

    - id: llama.cpp/SmolLM2-360M-Instruct-Q8_0-GGUF
      backend: llamacpp
      load_mode: lazy
      llamacpp:
        server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
        timeout_seconds: 30
        model_name: "SmolLM2-360M-Instruct"
        # documentation-only:
        model_path: "/Users/chranama/models/gguf/smollm2-360m-instruct/Q8_0.gguf"
      capabilities: { generate: true, extract: false }
      notes: "Host llama-server (llama.cpp) GGUF: SmolLM2 360M Instruct Q8_0 (smallest local baseline)"

profiles:
  host: {}

  docker-portable:
    default_model: meta-llama/Llama-3.2-1B-Instruct

  # Route generation to external llama-server (works for Demo A in docker or Demo B on host)
  llama-server:
    default_model: llama.cpp/SmolLM2-360M-Instruct-Q8_0-GGUF

    defaults:
      backend: llamacpp
      load_mode: lazy
      capabilities: { generate: true, extract: false }

      llamacpp:
        server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
        timeout_seconds: 30
        default_temperature: 0.7
        default_top_p: 0.95

    models:
      - id: llama.cpp/Qwen2.5-14B-Instruct-Q4_K_M-GGUF
        backend: llamacpp
        load_mode: lazy
        llamacpp:
          server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
          timeout_seconds: 30
          model_name: "Qwen2.5-14B-Instruct"
        capabilities: { generate: true, extract: false }
        notes: "External llama-server backend (Qwen2.5 14B Q4_K_M)"

      - id: llama.cpp/Meta-Llama-3.1-8B-Instruct-Q4_K_M-GGUF
        backend: llamacpp
        load_mode: lazy
        llamacpp:
          server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
          timeout_seconds: 30
          model_name: "Meta-Llama-3.1-8B-Instruct"
        capabilities: { generate: true, extract: false }
        notes: "External llama-server backend (Llama 3.1 8B Q4_K_M)"

      - id: llama.cpp/TinyLlama-1.1B-Chat-Q4_K_M-GGUF
        backend: llamacpp
        load_mode: lazy
        llamacpp:
          server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
          timeout_seconds: 30
          model_name: "TinyLlama-1.1B-Chat"
        capabilities: { generate: true, extract: false }
        notes: "External llama-server backend (TinyLlama 1.1B Chat Q4_K_M)"

      - id: llama.cpp/SmolLM2-360M-Instruct-Q8_0-GGUF
        backend: llamacpp
        load_mode: lazy
        llamacpp:
          server_url: ${LLAMA_SERVER_URL:-http://127.0.0.1:8081}
          timeout_seconds: 30
          model_name: "SmolLM2-360M-Instruct"
        capabilities: { generate: true, extract: false }
        notes: "External llama-server backend (SmolLM2 360M Instruct Q8_0)"

  test:
    default_model: fake
    defaults:
      backend: transformers
      load_mode: lazy
      capabilities: { generate: true, extract: true }
    models:
      - id: fake
        backend: transformers
        load_mode: lazy
        capabilities: { generate: true, extract: true }
        notes: "Fake model for integration tests"