# ================================
# Core API Settings
# ================================

# Secret used only by your UI → API requests.
# After starting the stack, run:
#   make seed-key API_KEY=<this value>
API_KEY=change_me_please_32chars_min

# LLM device selection for local mode (cpu, mps, cuda)
MODEL_DEVICE=cpu

# Optional default model override for uv_local mode
DEFAULT_MODEL_ID=mistralai/Llama-3.1-8B-Instruct


# ================================
# Database (Postgres)
# ================================
POSTGRES_USER=llm
POSTGRES_PASSWORD=llm
POSTGRES_DB=llm
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# SQLAlchemy connection URL (do not change unless needed)
DATABASE_URL=postgresql+asyncpg://llm:llm@postgres:5432/llm


# ================================
# Redis (Optional hot cache)
# ================================
REDIS_ENABLED=false
REDIS_URL=redis://llm_redis:6379/0


# ================================
# Rate Limits
# ================================
# Requests per minute (per API key)
RATE_LIMIT_RPM=60

# Maximum concurrent model generations
MAX_CONCURRENT_REQUESTS=4


# ================================
# Completion Cache
# ================================
# TTL (in seconds) for cache entries stored in Postgres
COMPLETION_CACHE_TTL_SECONDS=86400


# ================================
# Quotas
# ================================
# Monthly output-token quota (per API key)
# Set to 0 to disable quota enforcement
MONTHLY_TOKEN_QUOTA=0


# ================================
# Models (multi-model deployment)
# ================================
# For a single-model deployment:
#   leave MODELS_FILE empty → use defaults based on runtime (local vs docker)
MODELS_FILE=models.yaml


# ================================
# UI → API Integration
# ================================
# This must match API_KEY above so the UI playground works automatically.
VITE_API_KEY=change_me_please_32chars_min


# ================================
# Misc
# ================================
LOG_LEVEL=info