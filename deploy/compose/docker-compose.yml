# deploy/compose/docker-compose.yml
#

# ==========================
# Templates / extension fields
# ==========================

# ---- Server env (deep-merge safe) ----
x-server-env: &server_env
  APP_ROOT: "/app"
  APP_CONFIG_PATH: "/app/config/server.yaml"

  # âœ… Select config/server.yaml profile for containers
  APP_PROFILE: ${APP_PROFILE:-docker}

  # âœ… Select config/models.yaml profile (separate from APP_PROFILE)
  MODELS_PROFILE: ${MODELS_PROFILE:-}

  # âœ… Explicit DB wiring identity for readiness/debug ("docker" | "host" | "itest")
  DB_INSTANCE: ${DB_INSTANCE:-docker}

  # infra toggles + endpoints
  REDIS_ENABLED: ${REDIS_ENABLED:-0}
  REDIS_URL: ${REDIS_URL:-}

  # ðŸ”“ Make optional (no compose-time hard failure)
  DATABASE_URL: ${DATABASE_URL:-}

  WORKERS: ${WORKERS:-1}
  UVICORN_RELOAD: ${UVICORN_RELOAD:-0}

  MODELS_YAML: ${MODELS_YAML:-/app/config/models.yaml}

  POLICY_DECISION_PATH: ${POLICY_DECISION_PATH:-/app/policy_out/latest.json}
  SLO_OUT_DIR: ${SLO_OUT_DIR:-/app/slo_out}

  API_KEY: ${API_KEY:-}
  HUGGINGFACE_HUB_TOKEN: ${HUGGINGFACE_HUB_TOKEN:-}

  SCHEMAS_DIR: ${SCHEMAS_DIR:-}

  CONTAINER_MEMORY_BYTES: ${CONTAINER_MEMORY_BYTES:-}

  # llama-server wiring (Option A external llama.cpp)
  # âœ… Default to in-compose llama_server service (works for Demo A)
  LLAMA_SERVER_URL: ${LLAMA_SERVER_URL:-http://llama_server:8080}
  LLAMA_SERVER_API_KEY: ${LLAMA_SERVER_API_KEY:-}

# ---- GPU server env (deep-merge safe) ----
x-server-gpu-env: &server_gpu_env
  <<: *server_env
  MODEL_DEVICE: ${MODEL_DEVICE:-gpu}
  REDIS_ENABLED: ${REDIS_ENABLED:-1}
  UVICORN_RELOAD: "0"
  MODELS_PROFILE: ${MODELS_PROFILE_GPU:-docker}

# ---- Server base (uses env anchor) ----
x-server-base: &server_base
  build:
    context: ../..
    dockerfile: deploy/docker/Dockerfile.server
  restart: "no"
  environment:
    <<: *server_env
  depends_on:
    postgres:
      condition: service_healthy
    redis:
      condition: service_started

x-postgres-base: &postgres_base
  image: postgres:16-alpine
  restart: unless-stopped
  environment:
    POSTGRES_USER: ${POSTGRES_USER:-llm}
    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-llm}
    POSTGRES_DB: ${POSTGRES_DB:-llm}
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-llm} -d ${POSTGRES_DB:-llm}"]
    interval: 5s
    timeout: 3s
    retries: 10

x-redis-base: &redis_base
  image: redis:7-alpine
  restart: unless-stopped
  command: ["redis-server", "--save", "", "--appendonly", "no"]
  healthcheck:
    test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
    interval: 5s
    timeout: 3s
    retries: 10

# ---- UI template (dockerized always) ----
x-ui-base: &ui_base
  build:
    context: ../..
    dockerfile: deploy/docker/Dockerfile.ui
    args:
      VITE_API_KEY: ${API_KEY}
      # âœ… Browser-safe default: UI running in browser calls published API on localhost.
      VITE_API_BASE_URL: ${VITE_API_BASE_URL:-http://localhost:${API_PORT:-8000}}
  restart: unless-stopped


services:
  # ==========================
  # Data layer (INTERNAL-ONLY)
  # ==========================
  postgres:
    <<: *postgres_base
    profiles: ["infra"]
    expose:
      - "5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  redis:
    <<: *redis_base
    profiles: ["infra"]
    expose:
      - "6379"
    volumes:
      - redis_data:/data

  # ==========================
  # Data layer (HOST-PUBLISHED)
  # For running llm_server on the host (not in docker).
  # ==========================
  postgres_host:
    <<: *postgres_base
    profiles: ["infra-host"]
    ports:
      - "${POSTGRES_HOST_PORT:-5433}:5432"
    volumes:
      - pg_data_host:/var/lib/postgresql/data

  redis_host:
    <<: *redis_base
    profiles: ["infra-host"]
    ports:
      - "${REDIS_HOST_PORT:-6379}:6379"
    volumes:
      - redis_data_host:/data

  # ==========================
  # llama.cpp server (Option A external llama-server)
  #
  # IMPORTANT:
  # - Uses a prebuilt image by default (LLAMA_SERVER_IMAGE), so `up` is fast.
  # - Still supports compose rebuilds, with a pinned LLAMA_CPP_REF for determinism.
  # - Hard-fails at compose-time if LLAMA_MODEL_FILE is missing (when llama profile is enabled).
  # ==========================
  llama_server:
    profiles: ["llama"]
    image: ${LLAMA_SERVER_IMAGE:-llmep-llama-server:b8069}
    build:
      context: ../..
      dockerfile: deploy/docker/Dockerfile.llama-server
      args:
        LLAMA_CPP_REF: ${LLAMA_CPP_REF:-b8069}
    restart: unless-stopped
    environment:
      LLAMA_SERVER_PORT: ${LLAMA_SERVER_PORT:-8080}
      LLAMA_MODEL_FILE: ${LLAMA_MODEL_FILE:-}
      LLAMA_CTX_SIZE: ${LLAMA_CTX_SIZE:-4096}
      LLAMA_THREADS: ${LLAMA_THREADS:-}
      LLAMA_BATCH: ${LLAMA_BATCH:-256}
      LLAMA_UBATCH: ${LLAMA_UBATCH:-}
      LLAMA_N_GPU_LAYERS: ${LLAMA_N_GPU_LAYERS:-0}
      LLAMA_SEED: ${LLAMA_SEED:-0}
      LLAMA_TEMP: ${LLAMA_TEMP:-0.7}
      LLAMA_TOP_P: ${LLAMA_TOP_P:-0.95}
      LLAMA_MLOCK: ${LLAMA_MLOCK:-0}
      LLAMA_NO_MMAP: ${LLAMA_NO_MMAP:-0}
      LLAMA_SERVER_API_KEY: ${LLAMA_SERVER_API_KEY:-}
      LLAMA_PARALLEL: ${LLAMA_PARALLEL:-1}
    expose:
      - "8080"
    ports:
      - "${LLAMA_PUBLISH_PORT:-8080}:8080"
    volumes:
      # Host directory of GGUF models mounted read-only at /models
      - ${LLAMA_MODELS_DIR:-./.tmp/llama-models}:/models:ro
    healthcheck:
      test: ["CMD-SHELL", "/app/healthcheck.sh"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 20s
    mem_limit: ${LLAMA_MEM_LIMIT:-48g}
    memswap_limit: ${LLAMA_MEMSWAP_LIMIT:-48g}

  # ==========================
  # Integration test infra (ephemeral)
  # ==========================
  postgres_itest:
    image: postgres:16-alpine
    restart: "no"
    profiles: ["itest"]
    environment:
      POSTGRES_USER: ${ITEST_POSTGRES_USER:-llm}
      POSTGRES_PASSWORD: ${ITEST_POSTGRES_PASSWORD:-llm}
      POSTGRES_DB: ${ITEST_POSTGRES_DB:-llm_test}
    expose:
      - "5432"
    tmpfs:
      - /var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${ITEST_POSTGRES_USER:-llm} -d ${ITEST_POSTGRES_DB:-llm_test}"]
      interval: 5s
      timeout: 3s
      retries: 10

  redis_itest:
    image: redis:7-alpine
    restart: "no"
    profiles: ["itest"]
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    expose:
      - "6379"
    tmpfs:
      - /data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 5s
      timeout: 3s
      retries: 10

  # ==========================
  # Server (integration-test profile)
  # ==========================
  server_itest:
    <<: *server_base
    profiles: ["itest"]
    environment:
      <<: *server_env
      APP_PROFILE: ${APP_PROFILE:-docker}
      MODELS_PROFILE: ${MODELS_PROFILE_ITEST:-test}
      DB_INSTANCE: ${DB_INSTANCE_ITEST:-itest}

      MODEL_LOAD_MODE: off
      REQUIRE_MODEL_READY: "0"
      ENV: test

      # âœ… Keep a sensible default for itest
      DATABASE_URL: ${ITEST_DATABASE_URL:-postgresql+asyncpg://llm:llm@postgres_itest:5432/llm_test}

      REDIS_ENABLED: ${ITEST_REDIS_ENABLED:-1}
      REDIS_URL: ${ITEST_REDIS_URL:-redis://redis_itest:6379/0}
      MODEL_WARMUP: "0"
      POLICY_DECISION_PATH: ${ITEST_POLICY_DECISION_PATH:-/app/policy_out/latest.json}
    ports: []
    expose:
      - "8000"
    depends_on:
      postgres_itest:
        condition: service_healthy
      redis_itest:
        condition: service_healthy
    volumes:
      - ../../policy_out:/app/policy_out:rw
      - ../../slo_out:/app/slo_out:rw

  # ==========================
  # Server (CPU / portable)
  # ==========================
  server:
    <<: *server_base
    profiles: ["server"]
    ports:
      - "${API_PORT:-8000}:8000"
    environment:
      <<: *server_env
      APP_PROFILE: ${APP_PROFILE:-docker}
      MODELS_PROFILE: ${MODELS_PROFILE:-}
      # ðŸ”“ no longer required at compose-time
      DATABASE_URL: ${DATABASE_URL:-}
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ../../policy_out:/app/policy_out:rw
      - ../../slo_out:/app/slo_out:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/readyz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    mem_limit: ${SERVER_MEM_LIMIT:-12g}
    memswap_limit: ${SERVER_MEMSWAP_LIMIT:-12g}

  # ==========================
  # Server (CPU) + llama-server Demo A (explicit profile)
  # Ensures depends_on llama_server only when enabled.
  # ==========================
  server_llama:
    <<: *server_base
    profiles: ["server-llama"]
    ports:
      - "${API_PORT:-8000}:8000"
    environment:
      <<: *server_env
      APP_PROFILE: ${APP_PROFILE:-docker}
      MODELS_PROFILE: ${MODELS_PROFILE:-llama-server}
      LLAMA_SERVER_URL: ${LLAMA_SERVER_URL:-http://llama_server:8080}
      # ðŸ”“ no longer required at compose-time
      DATABASE_URL: ${DATABASE_URL:-}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      llama_server:
        condition: service_healthy
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ../../policy_out:/app/policy_out:rw
      - ../../slo_out:/app/slo_out:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/readyz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    mem_limit: ${SERVER_MEM_LIMIT:-12g}
    memswap_limit: ${SERVER_MEMSWAP_LIMIT:-12g}

  # ==========================
  # Server (CPU) + HOST llama-server Demo B (explicit profile)
  #
  # Purpose:
  # - Run llm_server in docker
  # - Route generation to a llama-server running on the host
  # - Do NOT depend_on the in-compose llama_server service
  #
  # Notes:
  # - macOS/Windows: host.docker.internal works out of the box
  # - Linux: extra_hosts enables host.docker.internal via host-gateway
  # ==========================
  server_llama_host:
    <<: *server_base
    profiles: ["server-llama-host"]
    ports:
      - "${API_PORT:-8000}:8000"
    environment:
      <<: *server_env
      APP_PROFILE: ${APP_PROFILE:-docker}
      MODELS_PROFILE: ${MODELS_PROFILE:-llama-server}
      LLAMA_SERVER_URL: ${LLAMA_SERVER_URL:-http://host.docker.internal:8080}
      # ðŸ”“ no longer required at compose-time
      DATABASE_URL: ${DATABASE_URL:-}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ../../policy_out:/app/policy_out:rw
      - ../../slo_out:/app/slo_out:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/readyz || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    mem_limit: ${SERVER_MEM_LIMIT:-12g}
    memswap_limit: ${SERVER_MEMSWAP_LIMIT:-12g}

  # ==========================
  # Server (GPU)
  # ==========================
  server_gpu:
    <<: *server_base
    profiles: ["server-gpu"]
    environment:
      <<: *server_gpu_env
      APP_PROFILE: ${APP_PROFILE:-docker}
      # ðŸ”“ no longer required at compose-time
      DATABASE_URL: ${DATABASE_URL:-}
    ports: []
    expose:
      - "8000"
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ../../policy_out:/app/policy_out:rw
      - ../../slo_out:/app/slo_out:rw
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/modelz || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 600s
      retries: 10
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]
    mem_limit: ${SERVER_MEM_LIMIT:-12g}
    memswap_limit: ${SERVER_MEMSWAP_LIMIT:-12g}

  # ==========================
  # UI (single service)
  # ==========================
  ui:
    <<: *ui_base
    profiles: ["ui"]
    ports:
      - "${UI_PORT:-5173}:80"
    # IMPORTANT:
    # Do NOT depends_on server/server_llama here, because it will force-start the wrong server
    # when you choose only one of the profiles. The UI will still work as soon as the API is up.

  # ==========================
  # Admin
  # ==========================
  pgadmin:
    image: dpage/pgadmin4:8
    restart: unless-stopped
    profiles: ["admin"]
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: "False"
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "${PGADMIN_PORT:-5050}:80"

  # ==========================
  # Observability (docker server)
  # ==========================
  prometheus:
    image: prom/prometheus:v2.54.1
    restart: unless-stopped
    profiles: ["obs"]
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    volumes:
      - ../observability/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "${PROM_PORT:-9090}:9090"

  grafana:
    image: grafana/grafana:11.1.0
    restart: unless-stopped
    profiles: ["obs", "obs-host"]
    volumes:
      - grafana_data:/var/lib/grafana
      - ../observability/grafana/grafana/provisioning:/etc/grafana/provisioning
      - ../observability/grafana/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "${GRAFANA_PORT:-3000}:3000"

  prometheus_host:
    image: prom/prometheus:v2.54.1
    restart: unless-stopped
    profiles: ["obs-host"]
    command:
      - "--config.file=/etc/prometheus/prometheus.host.yml"
    volumes:
      - ../observability/prometheus/prometheus.host.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "${PROM_HOST_PORT:-9091}:9090"

  # ==========================
  # Eval
  # ==========================
  eval:
    build:
      context: ../..
      dockerfile: deploy/docker/Dockerfile.eval
    restart: "no"
    profiles: ["eval"]
    environment:
      API_BASE_URL: ${API_BASE_URL:-http://server:8000}
      EVAL_ARGS: ${EVAL_ARGS:-}
      EVAL_WRITE_LATEST: ${EVAL_WRITE_LATEST:-1}
      EVAL_OUT_PATH: ${EVAL_OUT_PATH:-}
    volumes:
      - ../..:/work:ro
      - ../../results:/work/results:rw
      - ../../eval_out:/work/eval_out:rw

  eval_host:
    build:
      context: ../..
      dockerfile: deploy/docker/Dockerfile.eval
    restart: "no"
    profiles: ["eval-host"]
    environment:
      API_BASE_URL: ${API_BASE_URL:-http://host.docker.internal:${API_PORT:-8000}}
      EVAL_ARGS: ${EVAL_ARGS:-}
      EVAL_WRITE_LATEST: ${EVAL_WRITE_LATEST:-1}
      EVAL_OUT_PATH: ${EVAL_OUT_PATH:-}
    volumes:
      - ../..:/work:ro
      - ../../results:/work/results:rw
      - ../../eval_out:/work/eval_out:rw

  # ==========================
  # Policy
  # ==========================
  policy:
    build:
      context: ../..
      dockerfile: deploy/docker/Dockerfile.policy
    restart: "no"
    profiles: ["policy"]
    environment:
      POLICY_RUN_DIR: ${POLICY_RUN_DIR:-latest}
      POLICY_MODELS_YAML: ${POLICY_MODELS_YAML:-/work/config/models.yaml}
      POLICY_THRESHOLDS_ROOT: ${POLICY_THRESHOLDS_ROOT:-/app/policy/src/llm_policy/thresholds}
      POLICY_THRESHOLD_PROFILE: ${POLICY_THRESHOLD_PROFILE:-extract/default}
      POLICY_GENERATE_PROFILE: ${POLICY_GENERATE_PROFILE:-generate/portable}
      POLICY_GENERATE_SLO_PATH: ${POLICY_GENERATE_SLO_PATH:-/work/slo_out/generate/latest.json}
      POLICY_OUT_PATH: ${POLICY_OUT_PATH:-/work/policy_out/latest.json}
    volumes:
      - ../..:/work:rw
      - ../../results:/work/results:rw
      - ../../config:/work/config:rw
      - ../../eval_out:/work/eval_out:rw
      - ../../policy_out:/work/policy_out:rw
      - ../../slo_out:/work/slo_out:rw
    working_dir: /work
    # NOTE:
    # No command here. Dockerfile.policy sets:
    #   ENTRYPOINT ["policy"]
    #   CMD ["--help"]
    # For smoke tests, invoke explicitly:
    #   docker compose --profile policy run --rm policy <subcommand> ...

volumes:
  pg_data:
  redis_data:
  pg_data_host:
  redis_data_host:
  grafana_data: