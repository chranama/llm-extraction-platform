# eval/pyproject.toml
[project]
name = "llm_eval"
version = "0.1.0"
description = "Evaluation and research toolkit for llm-server"
requires-python = "==3.12.*"

dependencies = [
  # HTTP + API interaction
  "httpx>=0.27",

  # Dataset handling
  "datasets>=2.21",

  # Schema validation / metrics
  "jsonschema>=4.21,<5.0",

  # Reporting / analytics
  "pandas>=2.2",
  "numpy>=1.26",

  # CLI / config
  "python-dotenv>=1.0.0",
  "pydantic-settings>=2.0",
  "llm-contracts",

  # âœ… Always available (used by some dataset adapters / viz workflows)
  "fiftyone>=1.0.0",
]

[project.scripts]
eval = "llm_eval.cli:main"

[project.optional-dependencies]
test = [
  "pytest>=8",
  "pytest-asyncio>=0.23",
  "pytest-cov>=5",
  "coverage>=7",
]

lint = [
  "ruff>=0.5",
  "black>=24",
]

viz = [
  "matplotlib>=3.9",
  "seaborn>=0.13",
]

# Convenience: install everything commonly used in this module
all = [
  "llm_eval[test,lint,viz]",
]

[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]
include = ["llm_eval*"]
exclude = ["tests*"]

[tool.setuptools.package-data]
llm_eval = ["**/*.yaml", "**/*.json"]

[tool.uv.sources]
llm-contracts = { path = "../contracts", editable = true }

[tool.pytest.ini_options]
addopts = "-q -ra --strict-markers"
testpaths = ["tests"]
asyncio_mode = "auto"
markers = [
  "unit: unit tests",
  "integration: tests that call a live llm-server",
]
